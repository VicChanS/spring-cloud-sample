version: '2'

services:
  mysql:
    container_name: mysql
    build:
      context: ./docker-db/mysql
      args:
        - MYSQL_DATABASE=default_database
        - MYSQL_USER=default_user
        - MYSQL_PASSWORD=secret
        - MYSQL_ROOT_PASSWORD=root
    volumes:
      - ./docker-db/data/mysql/:/var/lib/mysql
    # 使用ports会将端口暴露给主机和其他容器。
    # 由于本地测试需要连接本文件的容器，因此都设置为ports方便测试
    ports:
      - 3306:3306
    restart: "always" #重启策略，能够使服务保持始终运行，生产环境推荐使用
    # expose暴露容器给link到当前容器的容器，或者暴露给同一个networks的容器
    # expose不会将端口暴露给主机，主机无法访问expose的端口
    #expose:
    #  - 3306
    # mem_limit: 300000000 # 内存限制
    # 限制可用的 swap（交换内存）大小，它的值包含容器可用内存和可用 swap
    # 设置为 0 和不设置是一样的，此时如果设置了mem_limit，则该值默认为mem_limit的两倍
    # 设置为-1 则表示不受限
    #memswap_limit: 0
    # 设置容器使用 swap 的紧迫程度，默认会继承主机的 swappiness
    # 范围：0-100，值越大优先级越高。0表示不使用swap
    #mem_swappiness: 0

  redis:
    container_name: redis
    build:
      context: ./docker-db/redis
    ports:
      - 6379:6379
    restart: "always"
    volumes:
      - ./docker-db/data/redis:/data
    #mem_limit: 100000000 # 内存限制

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:3-management
    hostname: rabbitmq
    environment:
      - RABBITMQ_ERLANG_COOKIE=${RABBITMQ_ERLANG_COOKIE}
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_DEFAULT_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_DEFAULT_PASS}
      - RABBITMQ_DEFAULT_VHOST=${RABBITMQ_DEFAULT_VHOST}
    ports:
      - 15672:15672
      - 5672:5672
    restart: "always"
    #mem_limit: 100000000 # 内存限制

  zookeeper:
    container_name: zookeeper
    image: wurstmeister/zookeeper
    ports:
      - 2181:2181
    restart: "always"
    #mem_limit: 150000000 # 内存限制

  kafka:
    container_name: kafka
    build: ./docker-kafka
    ports:
      - 9092:9092
    restart: "always"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "test:1:1"
      # 设置堆栈大小
      KAFKA_HEAP_OPTS: -Xmx256M -Xms256M
      # 设置日志输出路径
      #KAFKA_LOG_DIRS: "/kafka_data"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      #- ./kafka_data:/kafka_data
    depends_on:
      - zookeeper
    #mem_limit: 500000000 # 内存限制

  zipkin:
    container_name: zipkin
    image: openzipkin/zipkin:latest
    environment:
      # 从kafka中获取信息。注意，在logstash中不要抓取zipkin主题的kafka信息，否则会导致zipkin无法正常获取kafka信息
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # 把链路信息保存到ES中
      - STORAGE_TYPE=elasticsearch
      # ES主机地址
      - ES_HOSTS=http://elasticsearch:9200
      # ES登录用户名
      - ES_USERNAME=elastic
      # ES登录密码
      - ES_PASSWORD=changeme
      #- JAVA_OPTS=-Xmx256M -Xms256M
      - JAVA_OPTS=-Xmx256M -Xms256M -Dlogging.level.zipkin=INFO -Dlogging.level.zipkin2=INFO
    ports:
      - 9411:9411
      - 1936:1936
    restart: "always"
    depends_on:
      - kafka
      - elasticsearch
    #mem_limit: 600000000 # 内存限制

  # zipkin的数据存入ES后，会丢失依赖关系，添加zipkin-dependencies可以解决
  # 可能是由于ES版本与dependency版本不一致或者内存不足的问题，未实现预期效果，待改进
#  zipkin-dependencies:
#    container_name: zipkin-dependencies
#    image: openzipkin/zipkin-dependencies:latest
#    # 启动定时任务
#    #entrypoint: crond -f
#    environment:
#      # 把链路信息保存到ES中
#      - STORAGE_TYPE=elasticsearch
#      # ES主机地址
#      - ES_HOSTS=http://elasticsearch:9200
#      # ES登录用户名
#      - ES_USERNAME=elastic
#      # ES登录密码
#      - ES_PASSWORD=changeme
#      #- ZIPKIN_LOG_LEVEL=DEBUG
#      # Uncomment to adjust memory used by the dependencies job
#      - JAVA_OPTS=-verbose:gc -Xmx1024M -Xms1024M
#    depends_on:
#      - elasticsearch

  elasticsearch:
    container_name: elasticsearch
    build:
      context: docker-elk/elasticsearch/
      args:
        ELK_VERSION: ${ELK_VERSION}
    volumes:
      - ./docker-elk/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
      - ./docker-elk/elasticsearch/data:/usr/share/elasticsearch/data
      - ./docker-elk/elasticsearch/plugins:/usr/share/elasticsearch/plugins
    ports:
      - 9200:9200
      - 9300:9300
    restart: "always"
    environment:
      ES_JAVA_OPTS: "-Xmx256m -Xms256m"
      ELASTIC_PASSWORD: changeme
    #mem_limit: 800000000 # 内存限制

  logstash:
    container_name: logstash
    build:
      context: docker-elk/logstash/
      args:
        ELK_VERSION: ${ELK_VERSION}
    volumes:
      - ./docker-elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./docker-elk/logstash/pipeline:/usr/share/logstash/pipeline:ro
      # docker-compose V3的语法
      #- type: bind
      #  source: ./docker-elk/logstash/config/logstash.yml
      #  target: /usr/share/logstash/config/logstash.yml
      #  read_only: true
      #- type: bind
      #  source: ./docker-elk/logstash/pipeline
      #  target: /usr/share/logstash/pipeline
      #  read_only: true
    ports:
      - 5000:5000
      - 9600:9600
    restart: "always"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    depends_on:
      - elasticsearch
      - kafka
    #mem_limit: 700000000 # 内存限制

  kibana:
    container_name: kibana
    build:
      context: docker-elk/kibana/
      args:
        ELK_VERSION: ${ELK_VERSION}
    volumes:
      - ./docker-elk/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
      # docker-compose V3的语法
      #- type: bind
      #  source: ./docker-elk/kibana/config/kibana.yml
      #  target: /usr/share/kibana/config/kibana.yml
      #  read_only: true
    ports:
      - 5601:5601
    restart: "always"
    depends_on:
      - elasticsearch
    #mem_limit: 500000000 # 内存限制

# 暂时不需要单独设置网络，default默认网络默认就是bridge模式
#networks:
#  elk:
#    # 桥接模式，可以和所有容器互通数据（包括宿主机）,需要配合ports或expose使用
#    driver: host